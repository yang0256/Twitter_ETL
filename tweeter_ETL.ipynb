{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyONQxdNuNW6Z8SG4pk6SPcW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yang0256/Twitter_ETL/blob/main/tweeter_ETL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "!pip install transformers\n",
        "# Load the FinBERT model\n",
        "from transformers import TFBertForSequenceClassification, BertTokenizer"
      ],
      "metadata": {
        "id": "ShbKQvqMsy_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "txhjot7XfB4K",
        "outputId": "1d7dc31c-1d86-4cf1-fd43-f92e92bbccef"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-4fb0a20ed293>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1800\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Sleep for 30 minutes (30 * 60 seconds)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# tweets collector\n",
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "# Set the Bearer Token\n",
        "bearer_token = \"AAAAAAAAAAAAAAAAAAAAAN%2FhlQEAAAAAA3KWaOM4ynnWjSNqd%2BH4ncV6mvU%3DRG5HSsW7lgy9kJakzkn67aBPzmfnAHCELtudRJisaeeNUHlkoj\"\n",
        "search_url = \"https://api.twitter.com/2/tweets/search/recent\"\n",
        "query_params = {\"query\": \"from:business\", \"max_results\": 100, \"tweet.fields\": \"public_metrics\"}\n",
        "\n",
        "def bearer_oauth(r):\n",
        "    r.headers[\"Authorization\"] = f\"Bearer {bearer_token}\"\n",
        "    r.headers[\"User-Agent\"] = \"v2RecentSearchPython\"\n",
        "    return r\n",
        "\n",
        "def connect_to_endpoint(url, params):\n",
        "    response = requests.get(url, auth=bearer_oauth, params=params)\n",
        "    #print(response.status_code)\n",
        "    if response.status_code != 200:\n",
        "        raise Exception(response.status_code, response.text)\n",
        "    return response.json()\n",
        "\n",
        "\n",
        "# ML\n",
        "\n",
        "def process_texts(df):\n",
        "    # Load the pre-trained model and tokenizer\n",
        "    \n",
        "    model_name = 'ProsusAI/finbert'\n",
        "    BertTokenizer.from_pretrained(model_name)\n",
        "    model = TFBertForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "    # Initialize positive and negative scores\n",
        "    pro_positive = 0\n",
        "    pro_negative = 0\n",
        "\n",
        "    # Loop over the values in the \"text\" column and aggregate the scores\n",
        "    for text in df['text']:\n",
        "        # Tokenize the text\n",
        "        inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"tf\")\n",
        "\n",
        "        # Prepare the input\n",
        "        input_ids = inputs[\"input_ids\"]\n",
        "        attention_mask = inputs[\"attention_mask\"]\n",
        "\n",
        "        # Run inference\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        scores = outputs[0][0].numpy()\n",
        "        pro_positive += scores[1]\n",
        "        pro_negative += scores[0]\n",
        "\n",
        "    return pro_positive, pro_negative\n",
        "\n",
        "\n",
        "\n",
        "df_scores = pd.DataFrame(columns=['pro_positive', 'pro_negative'])\n",
        "\n",
        "def main():\n",
        "    json_response = connect_to_endpoint(search_url, query_params)\n",
        "    \n",
        "    tweets_df = pd.DataFrame(json_response['data'])\n",
        "    #print(tweets_df[0:1])\n",
        "    print(tweets_df[0:100]['text'])\n",
        "    \n",
        "    tweets_df.to_csv(\"tweets.csv\", index=False)\n",
        "\n",
        "    df = pd.read_csv('tweets.csv')\n",
        "    \n",
        "    pro_positive, pro_negative = process_texts(df)\n",
        "\n",
        "    # add value\n",
        "    data = {\n",
        "        'pro_positive': pro_positive,\n",
        "        'pro_negative': pro_negative\n",
        "    }\n",
        "    df_scores.loc[len(df_scores)] = data\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    while True:\n",
        "        main()\n",
        "        print(df_scores)\n",
        "        time.sleep(1800) # Sleep for 30 minutes (30 * 60 seconds)\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#presentation"
      ],
      "metadata": {
        "id": "_LN5qLaCKzPW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}